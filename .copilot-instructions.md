# EvoTribes — AI Development Instructions

## Project in One Sentence

EvoTribes is a modular, learning-first genetic multi-agent simulation built with Gymnasium to study emergent group behavior and alignment failures through fully documented, iterative development.

---

## Core Development Philosophy

This repository is not optimized for speed of code generation.

It is optimized for:

- understanding every component
- traceability of changes
- reproducibility
- behavioral analysis

The goal is to **learn**, not to produce opaque code.

---

## Context Loading Rules (CRITICAL)

To avoid polluting context:

DO NOT load all documentation.

Only read from `/docs` when the task explicitly requires it.

### Documentation Map

- `00_overview.md` → project goals and system architecture
- `01_environment.md` → environment rules and observation/action spaces
- `02_policies.md` → agent brain implementations
- `03_genetic_algorithm.md` → evolution logic
- `04_scenarios.md` → experimental configurations
- `05_metrics.md` → what must be logged and measured
- `06_alignment_cases.md` → reward hacking experiments

Load documentation **on demand**, not by default.

---

## Repository Structure

### `/src`

Contains all core logic.

Never place core logic in `scripts/`.

Submodules:

- `envs/` → Gymnasium environments and rendering
- `policies/` → agent decision systems (MLP, tree, etc.)
- `evolution/` → genetic algorithm and genome handling
- `metrics/` → logging and behavior measurements
- `ui/` → interactive controls for the simulation

---

### `/scripts`

Thin entry points only.

Their job is to call into `src/`.

Examples:

- run training
- launch demo
- run parameter sweeps

---

### `/docs`

Human-readable system knowledge.

Used only when needed.

Never duplicate documentation inside code.

**Substructure:**

- **Top-level docs** (`00_overview.md` … `06_alignment_cases.md`) → architectural specs, loaded on-demand
- **`/docs/notes`** → detailed iteration notes, one per iteration

---

### `/docs/notes` — Iteration Notes System

**Critical:** After completing any iteration, you MUST create a detailed note
in `docs/notes/iteration_XX.md` using the template in `TEMPLATE.md`.

Iteration notes serve as the **single source of truth** for what was built,
why it was built, and how it works.

**Required sections:**

1. Goal & rationale
2. What was implemented (file-by-file breakdown)
3. Architecture & design decisions
4. Mathematical considerations (formulas + plain-language explanations)
5. Algorithms & logic (step-by-step)
6. Key concepts explained for beginners
7. **Concrete examples** (code snippets, walkthrough scenarios, sample outputs)
8. Configuration parameters
9. How to run & what to expect
10. Known limitations & bugs
11. Docs modified
12. Test coverage
13. Version history
14. Next iteration preview
15. Questions & open issues

**Why?** The user reads these notes after each iteration to understand the
current state and spot bugs. They are beginner-friendly and exhaustively
detailed.

**Critical: Always include examples.** For every technical concept, formula,
or algorithm, provide at least one concrete example showing:

- Input values
- Step-by-step computation
- Output or result
- What it means in the context of the simulation

Examples make abstract concepts tangible and help catch bugs early.

---

### `/tests`

Smoke tests and shape validation.

Every new system component must have at least one validation test or runnable check.

---

## Iteration Workflow (MANDATORY)

For every significant change:

1. Start with a short explanation of **WHY** the change exists.
2. Deliver a **small, runnable iteration**.
3. List **all modified or created files**.
4. Generate **FULL FILES**, not partial snippets, for major features.
5. Update the relevant documentation.
6. Create a detailed iteration note in `docs/notes/iteration_XX.md`.
7. Update `VERSION` and `CHANGELOG.md`.
8. Provide:
   - run command
   - expected observable behavior
   - validation method

---

## Versioning & Change Tracking

### VERSION File

The `VERSION` file at the project root contains the current version number
(e.g., `0.1.0`).

**Semantic versioning:**

- **MAJOR** (X.0.0) — breaking changes to the API or environment
- **MINOR** (0.X.0) — new features, backward-compatible
- **PATCH** (0.0.X) — bug fixes

**When to bump:**

- After completing an iteration, increment the MINOR version.
- If a change breaks existing code or configs, increment MAJOR.
- For small fixes within an iteration, increment PATCH.

### CHANGELOG.md

Every version bump must add an entry to `CHANGELOG.md`.

Follow [Keep a Changelog](https://keepachangelog.com/) format:

```markdown
## [0.X.0] — YYYY-MM-DD

### Added

- Feature 1
- Feature 2

### Changed

- Modification 1

### Fixed

- Bug fix 1
```

### Automatic Git Tagging

The GitHub Actions workflow (`.github/workflows/ci.yml`) automatically
creates a Git tag when `VERSION` changes and is pushed to `main`.

Do not manually create version tags unless the workflow fails.

---

## Code Generation Rules

Code must be:

- simple
- modular
- heavily readable
- minimally dependent on external libraries
- configurable (no hidden constants)
- **well-exemplified** (docstrings include usage examples)

All parameters must live in a config or scenario system.

No hardcoded magic values.

**Documentation style:**

- Every complex function should have a docstring with an example
- Every formula should be followed by a worked example
- Every algorithm should show one complete iteration with sample data

---

## Genetic Algorithm Rules

The evolution loop must remain:

1. Evaluate
2. Select
3. Crossover
4. Mutate
5. Next generation

This must be transparent and inspectable.

---

## Policy System Rules

All agent brains must implement a shared interface.

Swapping policy implementations must not require changes elsewhere.

---

## Documentation Synchronization

After any architectural change:

Update the relevant file in `/docs`.

Documentation is part of the system, not an afterthought.

---

## Metrics and Observability

Every training run must produce:

- logged metrics
- behavioral indicators
- reproducible outputs

Saved in a `/runs` directory.

---

## Behavioral Modeling Safety Rule

Do not implement self-harm behaviors.

If modeling collapse or destructive optimization, use safe abstractions:

- withdrawal
- freezing
- isolation
- surrender
- score sacrifice

---

## Dependencies & Requirements

All Python dependencies are listed in `requirements.txt`.

When adding a new dependency:

1. Add it to `requirements.txt` with a version constraint.
2. Document why it's needed in a comment.
3. If it's for a future feature, comment out the line and note which iteration will need it.

**Current dependencies:**

- `gymnasium` — environment interface
- `numpy` — array operations
- `pygame` — rendering
- `pytest` — testing

**Planned (future iterations):**

- `torch` — neural network policies (Iteration 2+)
- `matplotlib` — plotting metrics (Iteration 4+)
- `pandas` — data analysis (Iteration 4+)

---

## When Unsure

If information is missing:

Make a reasonable assumption,
state it explicitly,
and proceed.

Do not block iteration.

---

## Definition of Done for Any Feature

A feature is complete only if:

- it runs
- it is documented (both in code and in the relevant `/docs` file)
- it has a detailed iteration note in `docs/notes/`
- it is tested (at least one smoke test)
- it is measurable (produces observable outputs or logs)
- it is explainable (formulas, algorithms, and concepts documented)
- `VERSION` and `CHANGELOG.md` are updated
